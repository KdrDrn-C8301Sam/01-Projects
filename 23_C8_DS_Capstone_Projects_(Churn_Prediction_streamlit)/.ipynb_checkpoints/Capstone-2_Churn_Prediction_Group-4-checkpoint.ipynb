{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mg_ZAS0B2slE"
   },
   "source": [
    "___\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EjVhtzq2slH"
   },
   "source": [
    "# WELCOME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqV3cXW-2slL"
   },
   "source": [
    "Welcome to \"***Employee Churn Analysis Project***\". This is the second project of Capstone Project Series, which you will be able to build your own classification models for a variety of business settings. \n",
    "\n",
    "Also you will learn what is Employee Churn?, How it is different from customer churn, Exploratory data analysis and visualization of employee churn dataset using ***matplotlib*** and ***seaborn***, model building and evaluation using python ***scikit-learn*** package. \n",
    "\n",
    "You will be able to implement classification techniques in Python. Using Scikit-Learn allowing you to successfully make predictions with the Random Forest, Gradient Descent Boosting , KNN algorithms.\n",
    "\n",
    "At the end of the project, you will have the opportunity to deploy your model using *Streamlit*.\n",
    "\n",
    "Before diving into the project, please take a look at the determines and project structure.\n",
    "\n",
    "- NOTE: This project assumes that you already know the basics of coding in Python and are familiar with model deployement as well as the theory behind K-Means, Gradient Boosting , KNN, Random Forest, and Confusion Matrices. You can try more models and methods beside these to improve your model metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oRnVXpS2slN"
   },
   "source": [
    "# #Determines\n",
    "In this project you have HR data of a company. A study is requested from you to predict which employee will churn by using this data.\n",
    "\n",
    "The HR dataset has 14,999 samples. In the given dataset, you have two types of employee one who stayed and another who left the company.\n",
    "\n",
    "You can describe 10 attributes in detail as:\n",
    "- ***satisfaction_level:*** It is employee satisfaction point, which ranges from 0-1.\n",
    "- ***last_evaluation:*** It is evaluated performance by the employer, which also ranges from 0-1.\n",
    "- ***number_projects:*** How many of projects assigned to an employee?\n",
    "- ***average_monthly_hours:*** How many hours in averega an employee worked in a month?\n",
    "- **time_spent_company:** time_spent_company means employee experience. The number of years spent by an employee in the company.\n",
    "- ***work_accident:*** Whether an employee has had a work accident or not.\n",
    "- ***promotion_last_5years:*** Whether an employee has had a promotion in the last 5 years or not.\n",
    "- ***Departments:*** Employee's working department/division.\n",
    "- ***Salary:*** Salary level of the employee such as low, medium and high.\n",
    "- ***left:*** Whether the employee has left the company or not.\n",
    "\n",
    "First of all, to observe the structure of the data, outliers, missing values and features that affect the target variable, you must use exploratory data analysis and data visualization techniques. \n",
    "\n",
    "Then, you must perform data pre-processing operations such as ***Scaling*** and ***Label Encoding*** to increase the accuracy score of Gradient Descent Based or Distance-Based algorithms. you are asked to perform ***Cluster Analysis*** based on the information you obtain during exploratory data analysis and data visualization processes. \n",
    "\n",
    "The purpose of clustering analysis is to cluster data with similar characteristics. You are asked to use the ***K-means*** algorithm to make cluster analysis. However, you must provide the K-means algorithm with information about the number of clusters it will make predictions. Also, the data you apply to the K-means algorithm must be scaled. In order to find the optimal number of clusters, you are asked to use the ***Elbow method***. Briefly, try to predict the set to which individuals are related by using K-means and evaluate the estimation results.\n",
    "\n",
    "Once the data is ready to be applied to the model, you must ***split the data into train and test***. Then build a model to predict whether employees will churn or not. Train your models with your train set, test the success of your model with your test set. \n",
    "\n",
    "Try to make your predictions by using the algorithms ***Gradient Boosting Classifier***, ***K Neighbors Classifier***, ***Random Forest Classifier***. You can use the related modules of the ***scikit-learn*** library. You can use scikit-learn ***Confusion Metrics*** module for accuracy calculation. You can use the ***Yellowbrick*** module for model selection and visualization.\n",
    "\n",
    "In the final step, you will deploy your model using Streamlit tool.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97xzRLNj2slO"
   },
   "source": [
    "# #Tasks\n",
    "\n",
    "#### 1. Exploratory Data Analysis\n",
    "\n",
    "#### 2. Data Visualization\n",
    "\n",
    "#### 3. Data Pre-Processing\n",
    "\n",
    "#### 4. Cluster Analysis\n",
    "\n",
    "#### 5. Model Building\n",
    "\n",
    "#### 6. Model Deployement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLTGi7q02slP"
   },
   "source": [
    "## 1. Exploratory Data Analysis\n",
    "\n",
    "- Importing Modules\n",
    "- Loading Dataset\n",
    "- Data Insigts\n",
    "----------\n",
    "Exploratory Data Analysis is an initial process of analysis, in which you can summarize characteristics of data such as pattern, trends, outliers, and hypothesis testing using descriptive statistics and visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyrWBiyM2sld"
   },
   "source": [
    "### Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TI19sGjE2slf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyforest\n",
    "\n",
    "# 1-Import Libraies\n",
    "\n",
    "import pandas_profiling\n",
    "import pyforest\n",
    "\n",
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.ticker as mticker\n",
    "import squarify as sq\n",
    "\n",
    "# Importing plotly and cufflinks in offline mode\n",
    "import plotly \n",
    "import plotly.express as px\n",
    "import cufflinks as cf\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "from plotly.offline import iplot\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "\n",
    "# !pip install termcolor\n",
    "import colorama\n",
    "from colorama import Fore, Style  # makes strings colored\n",
    "from termcolor import colored\n",
    "from termcolor import cprint\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import missingno as msno \n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "from pyclustertend import hopkins\n",
    "\n",
    "import optuna\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor \n",
    "from sklearn.ensemble import ExtraTreesRegressor, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, f_regression, mutual_info_regression\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, LogisticRegression\n",
    "from sklearn.metrics import plot_confusion_matrix, r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import make_scorer, precision_score, precision_recall_curve, plot_precision_recall_curve \n",
    "from sklearn.metrics import plot_roc_curve, roc_auc_score, roc_curve, f1_score, accuracy_score, recall_score\n",
    "from sklearn.metrics import silhouette_samples,silhouette_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, KFold, cross_val_predict, train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score, cross_validate\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, scale, StandardScaler, RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, PolynomialFeatures, PowerTransformer  \n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree \n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier, plot_importance\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn(\"this will not show\")\n",
    "\n",
    "# Figure&Display options\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "pd.set_option('max_colwidth',200)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some Useful Functions\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def missing_values(df):\n",
    "    missing_number = df.isnull().sum().sort_values(ascending = False)\n",
    "    missing_percent = (df.isnull().sum() / df.isnull().count()).sort_values(ascending = False)\n",
    "    missing_values = pd.concat([missing_number, missing_percent], axis = 1, keys = ['Missing_Number', 'Missing_Percent'])\n",
    "    return missing_values[missing_values['Missing_Number'] > 0]\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def first_looking(df):\n",
    "    print(colored(\"Shape:\", attrs=['bold']), df.shape,'\\n', \n",
    "          colored('*'*100, 'red', attrs = ['bold']),\n",
    "          colored(\"\\nInfo:\\n\", attrs = ['bold']), sep = '')\n",
    "    print(df.info(), '\\n', \n",
    "          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n",
    "    print(colored(\"Number of Uniques:\\n\", attrs = ['bold']), df.nunique(),'\\n',\n",
    "          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n",
    "    print(colored(\"Missing Values:\\n\", attrs=['bold']), missing_values(df),'\\n', \n",
    "          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n",
    "    print(colored(\"All Columns:\", attrs = ['bold']), list(df.columns),'\\n', \n",
    "          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n",
    "\n",
    "    df.columns = df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')\n",
    "    print(colored(\"Columns after rename:\", attrs = ['bold']), list(df.columns),'\\n',\n",
    "          colored('*'*100, 'red', attrs = ['bold']), sep = '')  \n",
    "    print(colored(\"Columns after rename:\", attrs = ['bold']), list(df.columns),'\\n',\n",
    "          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n",
    "    print(colored(\"Descriptive Statistics \\n\", attrs = ['bold']), df.describe().round(2),'\\n',\n",
    "          colored('*'*100, 'red', attrs = ['bold']), sep = '') # Gives a statstical breakdown of the data.\n",
    "    print(colored(\"Descriptive Statistics (Categorical Columns) \\n\", attrs = ['bold']), df.describe(include = object).T,'\\n',\n",
    "          colored('*'*100, 'red', attrs = ['bold']), sep = '') # Gives a statstical breakdown of the data.\n",
    "    \n",
    "def multicolinearity_control(df):\n",
    "    feature = []\n",
    "    collinear = []\n",
    "    for col in df.corr().columns:\n",
    "        for i in df.corr().index:\n",
    "            if (abs(df.corr()[col][i]) > .9 and abs(df.corr()[col][i]) < 1):\n",
    "                    feature.append(col)\n",
    "                    collinear.append(i)\n",
    "                    print(colored(f\"Multicolinearity alert in between:{col} - {i}\", \n",
    "                                  \"red\", attrs = ['bold']), df.shape,'\\n',\n",
    "                                  colored('*'*100, 'red', attrs = ['bold']), sep = '')\n",
    "\n",
    "def duplicate_values(df):\n",
    "    print(colored(\"Duplicate check...\", attrs = ['bold']), sep = '')\n",
    "    print(\"There are\", df.duplicated(subset = None, keep = 'first').sum(), \"duplicated observations in the dataset.\")\n",
    "    duplicate_values = df.duplicated(subset = None, keep = 'first').sum()\n",
    "    if duplicate_values > 0:\n",
    "        df.drop_duplicates(keep = 'first', inplace = True)\n",
    "        print(duplicate_values, colored(\" Duplicates were dropped!\"),'\\n',\n",
    "              colored('*'*100, 'red', attrs = ['bold']), sep = '')\n",
    "#     else:\n",
    "#         print(colored(\"There are no duplicates\"),'\\n',\n",
    "#               colored('*'*100, 'red', attrs = ['bold']), sep = '')     \n",
    "        \n",
    "# def drop_columns(df, drop_columns):\n",
    "#     if drop_columns != []:\n",
    "#         df.drop(drop_columns, axis = 1, inplace = True)\n",
    "#         print(drop_columns, 'were dropped')\n",
    "#     else:\n",
    "#         print(colored('We will now check the missing values and if necessary, the related columns will be dropped!', attrs = ['bold']),'\\n',\n",
    "#               colored('*'*100, 'red', attrs = ['bold']), sep = '')\n",
    "        \n",
    "def drop_null(df, limit):\n",
    "    print('Shape:', df.shape)\n",
    "    for i in df.isnull().sum().index:\n",
    "        if (df.isnull().sum()[i] / df.shape[0]*100) > limit:\n",
    "            print(df.isnull().sum()[i], 'percent of', i ,'null and were dropped')\n",
    "            df.drop(i, axis = 1, inplace = True)\n",
    "            print('new shape:', df.shape)       \n",
    "    print('New shape after missing value control:', df.shape)\n",
    "    \n",
    "###############################################################################\n",
    "\n",
    "# To view summary information about the columns\n",
    "\n",
    "def first_look(col):\n",
    "    print(\"column name    : \", col)\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"Per_of_Nulls   : \", \"%\", round(df[col].isnull().sum() / df.shape[0]*100, 2))\n",
    "    print(\"Num_of_Nulls   : \", df[col].isnull().sum())\n",
    "    print(\"Num_of_Uniques : \", df[col].nunique())\n",
    "    print(\"Duplicates     : \", df.duplicated(subset = None, keep = 'first').sum())\n",
    "    print(df[col].value_counts(dropna = False))\n",
    "    \n",
    "###############################################################################\n",
    "\n",
    "def fill_most(df, group_col, col_name):\n",
    "    '''Fills the missing values with the most existing value (mode) in the relevant column according to single-stage grouping'''\n",
    "    for group in list(df[group_col].unique()):\n",
    "        cond = df[group_col] == group\n",
    "        mode = list(df[cond][col_name].mode())\n",
    "        if mode != []:\n",
    "            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[cond][col_name].mode()[0])\n",
    "        else:\n",
    "            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[col_name].mode()[0])\n",
    "    print(\"Number of NaN : \",df[col_name].isnull().sum())\n",
    "    print(\"------------------\")\n",
    "    print(df[col_name].value_counts(dropna = False))\n",
    "    \n",
    "###############################################################################    \n",
    "# bar grafiğindeki değerlerin gösterilmesi\n",
    "# show values in bar graphic\n",
    "def show_values_on_bars(axs):\n",
    "    def _show_on_single_plot(ax):        \n",
    "        for p in ax.patches:\n",
    "            _x = p.get_x() + p.get_width() / 2\n",
    "            _y = p.get_y() + p.get_height()\n",
    "            value = '{:.2f}'.format(p.get_height())\n",
    "            ax.text(_x, _y, value, ha=\"center\") \n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _show_on_single_plot(ax)\n",
    "    else:\n",
    "        _show_on_single_plot(axs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vS9n2J9-2sln"
   },
   "source": [
    "### Loading Dataset\n",
    "\n",
    "Let's first load the required HR dataset using pandas's \"read_csv\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvS39ktq2slt"
   },
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('HR_Dataset.csv')\n",
    "df = df0.copy()\n",
    "df.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Qd_Mxw-2sl9"
   },
   "outputs": [],
   "source": [
    "first_looking(df)\n",
    "duplicate_values(df)\n",
    "print(colored(\"Shape:\", attrs = ['bold']), df.shape,'\\n', colored('*'*100, 'red', attrs = ['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***According to the basic examinations on the dataset;***\n",
    "\n",
    "- We have a classification problem.\n",
    "- We are going to make classification on the target variable \"left\".\n",
    "- And we will build a model to get the best classification on the \"left\" column.\n",
    "- Because of that we are going to look at the balance of \"left\" column.\n",
    "- The dataset has 10 columns and 11991 observations after dropping of duplicated observations.\n",
    "- 8 columns contain numerical values and 2 columns contain categorical values. \n",
    "- There seems to be no missing value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({'departments_': 'department'}, axis=1, inplace=True)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative code\n",
    "# last_column = df.pop('left')\n",
    "# df.insert(9, 'left', last_column)\n",
    "# df.head(1)\n",
    "\n",
    "df = df[['satisfaction_level', 'last_evaluation', 'number_project',\n",
    "       'average_montly_hours', 'time_spend_company', 'work_accident',\n",
    "       'promotion_last_5years', 'department', 'salary', 'left']]\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I want to move the 'left' column, which is my target column, from where it is to the end. In this way, I will work more comfortably psychologically :)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wc8t0m9u2sl2"
   },
   "source": [
    "### Data Insights\n",
    "\n",
    "In the given dataset, we have two types of employee one who stayed and another who left the company. So, we can divide data into two groups and compare their characteristics. Here, we can find the average of both the groups using groupby() and mean() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'left' Column-Target Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprint(\"Have a First Look to 'left' Column\",'green')\n",
    "first_look('left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['left'].value_counts().iplot(kind=\"bar\", title = '\"left\" Column Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, values = df['left'].value_counts(), \n",
    "             names = (df['left'].value_counts()).index, \n",
    "             title = '\"left\" Column Distribution')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['left']\n",
    "print(f'Percentage of left-1: % {round(y.value_counts(normalize=True)[1]*100,2)} --> \\\n",
    "({y.value_counts()[1]} observations for left-1)\\nPercentage of left-0: % {round(y.value_counts(normalize=True)[0]*100,2)} --> ({y.value_counts()[0]} observations for left-0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'left' column has binary type values.\n",
    "- We have an imbalanced data.\n",
    "- Almost 17% of the employees didn't continue with the company and left.\n",
    "- 1991 employees left.\n",
    "- Almost 83% of the employees continue with the company and didn't leave.\n",
    "- 10000 employees didn't leave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('left').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprint('Dataset describe results according to the \"left==1\" condition','green', 'on_red')\n",
    "df[df['left'] == 1].describe().T.style.background_gradient(subset = ['mean','min','50%', 'max'], cmap = 'RdPu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprint('Dataset describe results according to the \"left==0\" condition','green', 'on_red')\n",
    "df[df['left'] == 0].describe().T.style.background_gradient(subset = ['mean','min','50%', 'max'], cmap = 'RdPu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'satisfaction_level' Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cprint(\"Have a First Look to 'left' Column\",'green', 'on_red')\n",
    "first_look('satisfaction_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['satisfaction_level'].value_counts().iplot(kind=\"bar\", title = '\"satisfaction_level\" Column Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x = df['satisfaction_level'], color='left', marginal = \"box\", hover_data = df.columns, \n",
    "             title = 'satisfaction_level and left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['satisfaction_level'], df['left']).iplot(kind='bar', title = 'satisfaction_level and left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although it comes to mind that there should be a linear relationship between 'satisfaction_level' and 'left', it does not look like this on the graph. \n",
    "- Those with a 'satisfaction_level' value of around 0.1 are very likely to 'left'. \n",
    "- There is a significant increase in the number of those whose 'satisfaction_level' value is between 3.5 and 4.5 and 'left'. In fact, the number of left ones exceeds the notleft ones. \n",
    "- When the 'satisfaction_level' value is between 7 and 9, there is an increase in the number of those left. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normally we expect low satisfaction level for the employees who has left, so the part near to 0 on the x-axis is make sense.\n",
    "- Besides a group of employee who are not very decisive about their satisfaction level have also been left the company. This group may need extra motivation for employee loyalty. Because they are not so clear in their assesments about their future in the company.\n",
    "- Also a group of employee whose satisfaction level is above the avarage have been left the company. This does not make sense so this must be investigate deeply.\n",
    "\n",
    "There may be some other issues:\n",
    "\n",
    "a. The method of gathering this information may be wrong. So the assessment of satisfaction level and the resignings may not be directly proportional.\n",
    "\n",
    "b. The assessment may not be up to date. By the time the satisfaction level may be decreased so at the real time the satisfaction level of all resigning employees may be close to 0.\n",
    "\n",
    "c. Some of the employees may have hidden their true feelings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'last_evaluation' Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cprint(\"Have a First Look to 'last_evaluation' Column\",'green')\n",
    "first_look('last_evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['last_evaluation'].value_counts().iplot(kind=\"bar\", title = '\"last_evaluation\" Column Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x = df['last_evaluation'], color='left', marginal = \"box\", hover_data = df.columns, \n",
    "             title = 'last_evaluation and left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['last_evaluation'], df['left']).iplot(kind='bar', title = 'last_evaluation and left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the employees have been assessed above 0.4.\n",
    "- There is a local increase between 0.45-0.6 and 0.8-1 in 'last_evaluation' values, as in 'satisfaction_level' values. There is an increase in the number of people who quit their jobs in these intervals\n",
    "- The evaluation of the resigning employees are gather in two groups. First is around the 0.5 and the second is between 0.8 and 1.0.\n",
    "- Intensive work may couse the resign of high evaluated employees (second group). Because employer will be happy with performance of these staff, however it will be a burden for employee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(df, x = 'satisfaction_level', y = 'last_evaluation', color = 'left',\n",
    "               title = \"'satisfaction_level' & 'last_evaluation'\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It becomes meaningful when the satisfaction level of employees and the evaluation of the employer brings together.\n",
    "\n",
    "As seen in the graph; the resigning employees are grouping in three diferent clusters.\n",
    "\n",
    "1. First group has a satisfaction level of 0.4 and last evaluation of 0.5.  This group as not a clear idea about the company and the employer does not have a clear assessment about them. Other features affacting this group have to be investigate. What are the main questions of this group? Why they are confusing? What are the pros and cons of the company for these group? and so on...\n",
    "\n",
    "2. The second group has a low satisfaction even if the employer evaluated them with high degrees. Then what can be the main problem of this group?\n",
    "Intensive work with a low salary may affect this group.\n",
    "Or intensive work without promotion may cause.\n",
    "On the next steps workload and motivation factors of this group have to be investigated.\n",
    "\n",
    "3. The third group has a high satisfaction level and evaluation point as well. The density of this group is fewer than the others. The issues that triger the leave of this group need to be investigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.violin(df[['satisfaction_level', 'last_evaluation', 'left']], color = 'left', box = True, points='all',\n",
    "                title = \"'satisfaction_level' & 'last_evaluation'\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'number_project' Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprint(\"Have a First Look to 'number_project' Column\",'green')\n",
    "first_look('number_project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['number_project'].value_counts().iplot(kind=\"bar\", title = '\"number_project\" Column Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, values = df['number_project'].value_counts(), \n",
    "             names = (df['number_project'].value_counts()).index, \n",
    "             title = '\"number_project\" Column Distribution')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x = df['number_project'], color='left', marginal = \"box\", hover_data = df.columns, \n",
    "             title = 'number_project and left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['number_project'], df['left']).iplot(kind='bar', title = 'number_project and left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of leaving employees is higher among those who have only two projects during the period. This can be summed up as: \"the employees with only two projects feel worthless or emptied\". Because most of the employees work on three or four projects.\n",
    "\n",
    "With the 6th project, the number of resignings is getting over the number of ongoings. There are no ongoing staff members who were assigned to 7 projects.\n",
    "\n",
    "Working on more projects may cause intensive workload,  regarding to this the satisfaction level may decrease with the insufficient motivators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(df[df['number_project'] == 2], x = 'satisfaction_level', y = 'last_evaluation', color = 'left',\n",
    "               title = \"'satisfaction_level' & 'last_evaluation' when 'number_project' == 2\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the satisfaction level, evaluation score and the number of projects together;\n",
    "\n",
    "The group of undecideds who were evaluated as 0.5 are the group who worked on the only two projects. As aresult our hypothesis about this group is becoming more clear. As the employer does not assign enough projects to this group, he/she cannot evaluate their performance and they feel worthless. Therefore, they are unsure about their future in the company. This may lead them to leave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(df[df['number_project'] > 4], x = 'satisfaction_level', y = 'last_evaluation', color = 'left',\n",
    "               title = \"'satisfaction_level' & 'last_evaluation' when 'number_project' > 4\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(df[df['number_project'] == 7], x = 'satisfaction_level', y = 'last_evaluation', color = 'left',\n",
    "               title = \"'satisfaction_level' & 'last_evaluation' when 'number_project' == 7\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leaving employees who worked on more than four projects are the group two and three of the last_evaluation section.\n",
    "\n",
    "Especially most of the second group of last_evaluation section are worked on seven projects and left the company. So again our hypothesis about this group is now more definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'average_montly_hours' Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprint(\"Have a First Look to 'average_montly_hours' Column\",'green')\n",
    "first_look('average_montly_hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_montly_hours'].value_counts().iplot(kind=\"bar\", title = '\"average_montly_hours\" Column Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x = df['average_montly_hours'], color='left', marginal = \"box\", hover_data = df.columns, \n",
    "             title = 'average_montly_hours and left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['average_montly_hours'], df['left']).iplot(kind='bar', title = 'average_montly_hours and left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking at the 'average_montly_hours' values, there is a local increase in turnover in the 125-160 month working hours range and 210-290 monthly working hours. \n",
    "- Those who work more than 290 hours per month are more likely to quit their jobs than those who do not. \n",
    "- So the next question is \"The average monthly working hours are related to projects number or not?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,6))\n",
    "sns.lineplot(data = df, x = 'average_montly_hours', y = 'number_project', hue = 'left')\n",
    "plt.title(\"'average_montly_hours' & 'number_project'\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the graph above it is seen that the group working on two projects is working nearly 130-160 hours monthly. It can be assessed that they have only two simple projects that they don't need to work hard, so their loyalty is weak.\n",
    "\n",
    "Most of the employees are working 135-275 hours monthly. In this group usually the employees who get two or more than five projects leaving the company.\n",
    "By the increasing number of projects the avarage monhtly working hours and the number of resignings are increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,6))\n",
    "sns.lineplot(data = df, y = 'average_montly_hours', x = 'number_project')\n",
    "plt.title(\"'average_montly_hours' & 'number_project'\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The increasing of the avarage monthly hours according to number of projects is seen on the graph.\n",
    "\n",
    "The rate of increase is higher between two and three projects, and after five projects. So it is clearly define the number of resignings due to the working hours.\n",
    "\n",
    "**There need to be an adjustment about the project numbers, working hours and workload. The projects must be assigned to more employees. Also, better incentives must be offered to staff who are working hard.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'time_spend_company' Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprint(\"Have a First Look to 'time_spend_company' Column\",'green')\n",
    "first_look('number_project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['time_spend_company'].value_counts().iplot(kind=\"bar\", title = '\"time_spend_company\" Column Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, values = df['time_spend_company'].value_counts(), \n",
    "             names = (df['time_spend_company'].value_counts()).index, \n",
    "             title = '\"time_spend_company\" Column Distribution')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x = df['time_spend_company'], color='left', marginal = \"box\", hover_data = df.columns, \n",
    "             title = 'time_spend_company and left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['time_spend_company'], df['left']).iplot(kind='bar', title = 'time_spend_company and left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking at the 'time_spent_company' values, there is an increase in turnover in the 3rd working year, but this increase gradually decreases until the 6th working year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(df[df['left'] == 1], x = 'satisfaction_level', y = 'last_evaluation', color = 'time_spend_company',\n",
    "               title =\"'satisfaction_level' & 'last_evaluation'\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen on the graph, the employees are not able to make a clear assessment of the company during the first three years of their employment. This, coupled with the other factors, tends to lead to leaving the company after three years.\n",
    "\n",
    "By the fourth year, their workload increases and their satisfaction decreases.\n",
    "\n",
    "After the fifth year, they make an assessment, \"they will leave or not\".\n",
    "\n",
    "If they decide to continue in the company, they never consider leaving after the sixth year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(df[df['left'] == 0], x = 'satisfaction_level', y = 'last_evaluation', color = 'time_spend_company')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df, x = 'time_spend_company', y = 'average_montly_hours', title = \"'time_spend_company' & 'average_montly_hours'\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df, x = 'time_spend_company', y = 'number_project', title = \"'time_spend_company' & 'number_project'\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x = 'time_spend_company', color = 'number_project', title = \"'time_spend_company' & 'number_project'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Then how is the relation between workload and time spend in the company?***\n",
    "\n",
    "Third year staff has the most workload. After that year number of participated project is decreasing stepped. It is make sense. The experienced staff becoming team leader or manager position. That's why less of them can be assigned to projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'work_accident' Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprint(\"Have a First Look to 'work_accident' Column\",'green')\n",
    "first_look('work_accident')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['work_accident'].value_counts().iplot(kind=\"bar\", title = '\"work_accident\" Column Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, values = df['work_accident'].value_counts(), \n",
    "             names = (df['work_accident'].value_counts()).index, \n",
    "             title = '\"work_accident\" Column Distribution')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "px.histogram(df, x = df['work_accident'], color='left', marginal = \"box\", hover_data = df.columns, \n",
    "             title = 'work_accident and left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['work_accident'], df['left']).iplot(kind='bar', title = 'work_accident and left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x = df['average_montly_hours'], color='work_accident', title = 'work_accident and average_montly_hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x = df['time_spend_company'], color='work_accident', title = 'work_accident and time_spend_company')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'work_accident' column has binary type values.\n",
    "- Left ratios are similar between those who have had a work accident and those who have not. \n",
    "- It does not appear to be a determining factor. In fact, it can be said that the left rate of those who have had a work accident is proportionally lower. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'promotion_last_5years' Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprint(\"Have a First Look to 'promotion_last_5years' Column\",'green')\n",
    "first_look('promotion_last_5years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['promotion_last_5years'].value_counts().iplot(kind=\"bar\", title = '\"promotion_last_5years\" Column Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, values = df['promotion_last_5years'].value_counts(), \n",
    "             names = (df['promotion_last_5years'].value_counts()).index, \n",
    "             title = '\"promotion_last_5years\" Column Distribution')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x = df['promotion_last_5years'], color='left', marginal = \"box\", hover_data = df.columns, \n",
    "             title = 'promotion_last_5years and left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['promotion_last_5years'], df['left']).iplot(kind='bar', title = 'promotion_last_5years and left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.strip(df, x = 'satisfaction_level', y = 'last_evaluation', color = 'promotion_last_5years',\n",
    "               title = \"'satisfaction_level' & 'last_evaluation'\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x = df['time_spend_company'], color='promotion_last_5years', title = 'time_spend_company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df[df['promotion_last_5years'] == 1], x = df['time_spend_company'], title = 'promotion_last_5years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'promotion_last_5years' column has binary type values.\n",
    "- Receiving a promotion in the last 5 working years is not determinative in terms of leaving or continuing to work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'department' Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprint(\"Have a First Look to 'department' Column\",'green')\n",
    "first_look('department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['department'].value_counts().iplot(kind=\"bar\", title = '\"department\" Column Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, values = df['department'].value_counts(), \n",
    "             names = (df['department'].value_counts()).index, \n",
    "             title = '\"department\" Column Distribution')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dep = pd.DataFrame(pd.crosstab(df['department'], df['left']))\n",
    "df_dep.rename(columns = {0 : 'not_left', 1 : 'left'}, inplace = True)\n",
    "df_dep = df_dep.assign(total = lambda x: (x['not_left'] + x['left']))\n",
    "df_dep = df_dep.assign(left_percentage = lambda x: (x['left'] / x['total'] * 100))\n",
    "df_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x = df['department'], color='left', marginal = \"box\", hover_data = df.columns, \n",
    "             title = 'department and left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['department'], df['left']).iplot(kind='bar', title = 'department and left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is not observed that the departments worked alone have an effect on the left decision. \n",
    "- It is seen that the left percentages of the departments are similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**'salary' Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprint(\"Have a First Look to 'salary' Column\",'green')\n",
    "first_look('salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['salary'].value_counts().iplot(kind=\"bar\", title = '\"salary\" Column Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, values = df['salary'].value_counts(), \n",
    "             names = (df['salary'].value_counts()).index, \n",
    "             title = '\"salary\" Column Distribution')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x = df['salary'], color='left', marginal = \"box\", hover_data = df.columns, \n",
    "             title = 'salary and left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['salary'], df['left']).iplot(kind='bar', title = 'salary and left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is seen that the left percentages of the salary are similar. \n",
    "- Even if it is small, there is an increase in the form of high-medium-low according to the salary status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sonuçlar eklenecek\n",
    "\n",
    "- 'work_accident', 'promotion_last_5years', 'left' columns have binary type values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Let's go on with the examination of numerical and categorical columns.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical= df.drop(['left'], axis = 1).select_dtypes('number').columns\n",
    "\n",
    "categorical = df.select_dtypes('object').columns\n",
    "\n",
    "print('---------------------')\n",
    "print(f'Numerical Columns:  {df[numerical].columns}')\n",
    "print(f'Categorical Columns: {df[categorical].columns}')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical].describe().T.style.background_gradient(subset = ['mean','std','50%','count'], cmap = 'RdPu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical].iplot(kind = 'histogram', subplots = True, bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in numerical:\n",
    "    df[i].iplot(kind = 'box', title = i, boxpoints = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue = \"left\", corner = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "sns.heatmap (df.corr(), annot = True, fmt = '.2f', vmin = -1, vmax = 1)\n",
    "plt.xticks(rotation = 45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()['left'].sort_values().drop('left').iplot(kind = 'barh');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_temp = df.corr()\n",
    "\n",
    "count = 'Done'\n",
    "feature =[]\n",
    "collinear= []\n",
    "for col in df_temp.columns:\n",
    "    for i in df_temp.index:\n",
    "        if (df_temp[col][i] > .9 and df_temp[col][i] < 1) or (df_temp[col][i] < -.9 and df_temp[col][i] > -1) :\n",
    "                feature.append(col)\n",
    "                collinear.append(i)\n",
    "                print(Fore.RED + f'\\033[1mmulticolinearity alert in between\\033[0m {col} - {i}')\n",
    "        else:\n",
    "            print(f'For {col} and {i}, there is NO multicollinearity problem') \n",
    "\n",
    "print('\\033[1mThe number of strong corelated features:\\033[0m', count) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Based on the examinations made above,***\n",
    "\n",
    "- There is no multicollinearity problem among the features.\n",
    "- We have weak level correlation between the numerical features and the target column.\n",
    "- Also there is weak level correlation between the columns.\n",
    "- Target variable demonstrates a slight negative correlation with the variables of \"creditscore\", \"tenure\", \"numberofproducts\" 'hascrcard' and 'isactivemember', \n",
    "- Target variable demonstrates slight positive correlation with the variables of 'age', 'balance' and 'estimatedsalary\".\n",
    "- Age has more influence on the decision to leave the bank than the other columns.\n",
    "- It is noteworthy that those who left the bank are in the 45-65 age group. \n",
    "- The increase in the number of products negatively affects the decision to continue with the bank.\n",
    "- Another remarkable situation is the concentration of leaving the bank in the group where the number of products is 3 and 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PsO9Iew2smG"
   },
   "source": [
    "## 2. Data Visualization\n",
    "\n",
    "- Employees Left\n",
    "- Determine Number of Projects\n",
    "- Determine Time Spent in Company\n",
    "- Subplots of Features\n",
    "----------\n",
    "You can search for answers to the following questions using data visualization methods. Based on these responses, you can develop comments about the factors that cause churn.\n",
    "\n",
    "- How does the promotion status affect employee churn?\n",
    "- How does years of experience affect employee churn?\n",
    "- How does workload affect employee churn?\n",
    "- How does the salary level affect employee churn?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRQhFwtq2smI"
   },
   "source": [
    "### Employees Left\n",
    "\n",
    "Let's check how many employees were left?\n",
    "Here, you can plot a bar graph using Matplotlib. The bar graph is suitable for showing discrete variable counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aKWfFLk2smL"
   },
   "outputs": [],
   "source": [
    "cprint('\"left\" Column Distribution','green')\n",
    "df.left.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with plotly\n",
    "# df['left'].value_counts().iplot(kind=\"bar\", title = '\"left\" Column Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with seaborn\n",
    "# plt.figure(figsize = (7,5))\n",
    "# sns.countplot(data = df, x = 'left');\n",
    "# for index,value in enumerate(df.left.value_counts()):\n",
    "#     plt.text(index, value, f'{value}', ha = 'center', va = 'bottom', fontsize = 13);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprint('\"left\" Column Distribution','green')\n",
    "fig = plt.figure(figsize = (11,6))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(df.left.value_counts().index, df.left.value_counts().values, color = 'green')\n",
    "plt.title('\"left\" Column Distribution')   \n",
    "plt.xlabel('left') \n",
    "plt.ylabel('Number of Employees') \n",
    "for index,value in enumerate(df.left.value_counts()):\n",
    "    plt.text(index, value, f'{value}', ha = 'center', va = 'bottom', fontsize = 13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn6NHSZE2smY"
   },
   "source": [
    "### Number of Projects\n",
    "\n",
    "Similarly, you can also plot a bar graph to count the number of employees deployed on how many projects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprint('\"number_project\" Column Distribution','green')\n",
    "df.number_project.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with plotly\n",
    "# df['number_project'].value_counts().iplot(kind=\"bar\", title = '\"number_project\" Column Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with seaborn\n",
    "# plt.figure(figsize = (7,5))\n",
    "# sns.countplot(data = df, x = 'number_project');\n",
    "# for index,value in enumerate(df.left.value_counts()):\n",
    "#     plt.text(index, value, f'{value}', ha = 'center', va = 'bottom', fontsize = 13);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGyyJcUP2sma"
   },
   "outputs": [],
   "source": [
    "cprint('\"number_project\" Column Distribution','green')\n",
    "fig = plt.figure(figsize = (11,6))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "# x = df.number_project.value_counts().index\n",
    "# y = df.number_project.value_counts().values\n",
    "df.number_project.value_counts().plot(kind = \"bar\", color = \"orange\")\n",
    "plt.title('\"number_project\" Column Distribution')   \n",
    "plt.xlabel('number_project') \n",
    "plt.ylabel('Number of Employees')\n",
    "plt.xticks(rotation = 0)\n",
    "for index,value in enumerate(df.number_project.value_counts().sort_values(ascending=False)):\n",
    "    plt.text(index, value, f'{value}', ha = 'center', va = 'bottom', fontsize = 13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48X9SO4v2smj"
   },
   "source": [
    "### Time Spent in Company\n",
    "\n",
    "Similarly, you can also plot a bar graph to count the number of employees have based on how much experience?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprint('\"time_spend_company\" Column Distribution','green')\n",
    "df.time_spend_company.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with plotly\n",
    "# df['time_spend_company'].value_counts().iplot(kind=\"bar\", title = '\"time_spend_company\" Column Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with seaborn\n",
    "# plt.figure(figsize = (7,5))\n",
    "# sns.countplot(data = df, x = 'time_spend_company');\n",
    "# for index,value in enumerate(df.left.value_counts()):\n",
    "#     plt.text(index, value, f'{value}', ha = 'center', va = 'bottom', fontsize = 13);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cprint('\"time_spend_company\" Column Distribution','green')\n",
    "fig = plt.figure(figsize = (11,6))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "df.time_spend_company.value_counts().plot(kind = \"bar\", color = \"pink\")\n",
    "plt.title('\"time_spend_company\" Column Distribution')   \n",
    "plt.xlabel('time_spend_company') \n",
    "plt.ylabel('Number of Employees')\n",
    "plt.xticks(rotation = 0)\n",
    "for index,value in enumerate(df.time_spend_company.value_counts().sort_values(ascending=False)):\n",
    "    plt.text(index, value, f'{value}', ha = 'center', va = 'bottom', fontsize = 13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEbtBv3q2smq"
   },
   "source": [
    "### Subplots of Features\n",
    "\n",
    "You can use the methods of the matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gt8FWYQu2smu"
   },
   "outputs": [],
   "source": [
    "for i in df:\n",
    "    df[i].iplot(kind = 'histogram', subplots = True, bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36OyDJyx2sm2"
   },
   "source": [
    "## 3. Data Pre-Processing\n",
    "\n",
    "- Scaling\n",
    "- Label Encoding\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8-SVBoq2snA"
   },
   "source": [
    "#### Label Encoding\n",
    "\n",
    "Lots of machine learning algorithms require numerical input data, so you need to represent categorical columns in a numerical column. In order to encode this data, you could map each value to a number. e.g. Salary column's value can be represented as low:0, medium:1, and high:2. This process is known as label encoding, and sklearn conveniently will do this for you using LabelEncoder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop('left', axis = 1)\n",
    "df1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HXszRiq2sm4"
   },
   "outputs": [],
   "source": [
    "df1 = pd.get_dummies(df1, columns = ['department','salary'], drop_first = True)\n",
    "df1.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iN94C5P42sm4"
   },
   "source": [
    "#### Scaling\n",
    "\n",
    "Some machine learning algorithms are sensitive to feature scaling while others are virtually invariant to it. Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. Also distance algorithms like KNN, K-means, and SVM are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity.\n",
    "\n",
    "Scaling Types:\n",
    "- Normalization: Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n",
    "\n",
    "- Standardization: Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pVP9UBQ2snC"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df1)\n",
    "#Store it separately for clustering\n",
    "df1_scaled= scaler.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1Gp2f7q2snF"
   },
   "source": [
    "## 4. Cluster Analysis\n",
    "\n",
    "- Find the optimal number of clusters (k) using the elbow method for for K-means.\n",
    "- Determine the clusters by using K-Means then Evaluate predicted results.\n",
    "---------\n",
    "\n",
    "- Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.\n",
    "\n",
    "    [Cluster Analysis](https://en.wikipedia.org/wiki/Cluster_analysis)\n",
    "\n",
    "    [Cluster Analysis2](https://realpython.com/k-means-clustering-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWQx_bhw2snG"
   },
   "source": [
    "#### The Elbow Method\n",
    "\n",
    "- \"Elbow Method\" can be used to find the optimum number of clusters in cluster analysis. The elbow method is used to determine the optimal number of clusters in k-means clustering. The elbow method plots the value of the cost function produced by different values of k. If k increases, average distortion will decrease, each cluster will have fewer constituent instances, and the instances will be closer to their respective centroids. However, the improvements in average distortion will decline as k increases. The value of k at which improvement in distortion declines the most is called the elbow, at which we should stop dividing the data into further clusters.\n",
    "\n",
    "    [The Elbow Method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)\n",
    "\n",
    "    [The Elbow Method2](https://medium.com/@mudgalvivek2911/machine-learning-clustering-elbow-method-4e8c2b404a5d)\n",
    "\n",
    "    [KMeans](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1)\n",
    "\n",
    "Let's find out the groups of employees who left. You can observe that the most important factor for any employee to stay or leave is satisfaction and performance in the company. So let's bunch them in the group of people using cluster analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMLD4mr32snH"
   },
   "outputs": [],
   "source": [
    "hopkins(df1_scaled,df1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First : Get the Best KMeans \n",
    "ks = range(1,10)\n",
    "inertias=[]\n",
    "for k in ks :\n",
    "    # Create a KMeans clusters\n",
    "    kc = KMeans(n_clusters=k,random_state=1)\n",
    "    kc.fit(df1_scaled)\n",
    "    inertias.append(kc.inertia_)\n",
    "\n",
    "# Plot ks vs inertias\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('Number of clusters, k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.xticks(ks)\n",
    "plt.style.use('ggplot')\n",
    "plt.title('What is the Best Number for KMeans ?')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "kmeans = KMeans()\n",
    "visu = KElbowVisualizer(kmeans, k = (1,10))\n",
    "visu.fit(df1_scaled)\n",
    "visu.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssd =[]\n",
    "\n",
    "K = range(2,10)\n",
    "\n",
    "for k in K:\n",
    "    model = KMeans(n_clusters=k)\n",
    "    model.fit(df1_scaled)\n",
    "    ssd.append(model.inertia_)\n",
    "    print(f'Silhouette Score for {k} clusters: {silhouette_score(df1_scaled, model.labels_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "model_3 = KMeans(n_clusters = 3, random_state = 101)\n",
    "visualizer = SilhouetteVisualizer(model_3)\n",
    "visualizer.fit(df1_scaled)    # Fit the data to the visualizer\n",
    "visualizer.poof();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "model_4 = KMeans(n_clusters = 4, random_state = 101)\n",
    "visualizer = SilhouetteVisualizer(model_4)\n",
    "visualizer.fit(df1_scaled)    # Fit the data to the visualizer\n",
    "visualizer.poof();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_model = KMeans(n_clusters = 3, random_state = 101)\n",
    "k_means_model.fit_predict(df1_scaled)\n",
    "labels = k_means_model.labels_\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_clusters'] = labels\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_clusters'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, values = df['predicted_clusters'].value_counts(), \n",
    "             names = (df['predicted_clusters'].value_counts()).index, \n",
    "             title = 'Predicted_Clusters Distribution')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, values = df[df['left']==0]['predicted_clusters'].value_counts(), \n",
    "             names = df[df['left']==0]['predicted_clusters'].value_counts().index, \n",
    "             title = 'Predicted_Clusters_Almost_Lost Distribution')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, values = df[df['left']==1]['predicted_clusters'].value_counts(), \n",
    "             names = df[df['left']==1]['predicted_clusters'].value_counts().index, \n",
    "             title = 'Predicted_Clusters_Almost_Lost Distribution')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df['left'], \n",
    "            df['predicted_clusters']).iplot(kind=\"bar\", title = 'Compare (left vs predicted-clusters)',\n",
    "            xTitle = 'left & clusters', yTitle = 'counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('left').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cprint('Dataset describe results according to the \"left==1\" condition','green')\n",
    "df.groupby(['left', 'predicted_clusters']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['predicted_clusters'], \n",
    "            df['left']).iplot(kind=\"bar\", title = 'Compare (predicted-clusters vs left)',\n",
    "            xTitle = 'clusters & left', yTitle = 'counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.left.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cprint('Dataset describe results according to the \"left==1\" condition','green')\n",
    "df.groupby('predicted_clusters').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cprint('Dataset describe results according to the \"left==1\" condition','green')\n",
    "df.groupby(['predicted_clusters', 'left']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpmbaABr2snN"
   },
   "source": [
    "## 5. Model Building\n",
    "\n",
    "- Split Data as Train and Test set\n",
    "- Built Gradient Boosting Classifier, Evaluate Model Performance and Predict Test Data\n",
    "- Built K Neighbors Classifier and Evaluate Model Performance and Predict Test Data\n",
    "- Built Random Forest Classifier and Evaluate Model Performance and Predict Test Data\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAiUMdtI2snk"
   },
   "source": [
    "#### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92xg3rvR2snl"
   },
   "source": [
    "- Confusion Matrix : You can use scikit-learn metrics module for accuracy calculation. A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model. This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making.\n",
    "\n",
    "    [Confusion Matrix](https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9VeChm62snm"
   },
   "source": [
    "- Yellowbrick: Yellowbrick is a suite of visualization and diagnostic tools that will enable quicker model selection. It’s a Python package that combines scikit-learn and matplotlib. Some of the more popular visualization tools include model selection, feature visualization, classification and regression visualization\n",
    "\n",
    "    [Yellowbrick](https://www.analyticsvidhya.com/blog/2018/05/yellowbrick-a-set-of-visualization-tools-to-accelerate-your-model-selection-process/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYsKmaZd2snO"
   },
   "source": [
    "### Split Data as Train and Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6b_dTvA2snQ"
   },
   "source": [
    "Here, Dataset is broken into two parts in ratio of 70:30. It means 70% data will used for model training and 30% for model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S15Bpefl2snS"
   },
   "outputs": [],
   "source": [
    "df2 = df.drop('predicted_clusters', axis = 1)\n",
    "df2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.get_dummies(df2, columns = ['department','salary'], drop_first = True)\n",
    "df2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.drop('left', axis = 1)\n",
    "y = df2['left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, X_train, X_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    \n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"Test_Set\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Train_Set\")\n",
    "    print(classification_report(y_train,y_pred_train))\n",
    "    plot_confusion_matrix(model, X_test, y_test, cmap=\"plasma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(y_train, y_train_pred, y_test, y_pred):\n",
    "    \n",
    "    scores = {\"train_set\": {\"Accuracy\" : accuracy_score(y_train, y_train_pred),\n",
    "                            \"Precision\" : precision_score(y_train, y_train_pred),\n",
    "                            \"Recall\" : recall_score(y_train, y_train_pred),                          \n",
    "                            \"f1\" : f1_score(y_train, y_train_pred)},\n",
    "    \n",
    "              \"test_set\": {\"Accuracy\" : accuracy_score(y_test, y_pred),\n",
    "                           \"Precision\" : precision_score(y_test, y_pred),\n",
    "                           \"Recall\" : recall_score(y_test, y_pred),                          \n",
    "                           \"f1\" : f1_score(y_test, y_pred)}}\n",
    "    \n",
    "    return pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4d55Vek2snX"
   },
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8OkbOrC2snY"
   },
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_model = GradientBoostingClassifier(random_state = 101)\n",
    "GB_model.fit(X_train, y_train)\n",
    "y_pred = GB_model.predict(X_test)\n",
    "y_train_pred = GB_model.predict(X_train)\n",
    "\n",
    "GB_model_f1 = f1_score(y_test, y_pred)\n",
    "GB_model_acc = accuracy_score(y_test, y_pred)\n",
    "GB_model_recall = recall_score(y_test, y_pred)\n",
    "GB_model_auc = roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GB_Model\")\n",
    "print (\"------------------\")\n",
    "eval(GB_model, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val(y_train, y_train_pred, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassPredictionError\n",
    "\n",
    "visualizer = ClassPredictionError(GB_model)\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "visualizer.poof();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance for Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_feature_imp = pd.DataFrame(index=X.columns, data = GB_model.feature_importances_, columns = ['Importance']).sort_values(\"Importance\", ascending = False)\n",
    "GB_feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "sns.barplot(data = GB_feature_imp.sort_values('Importance', ascending = False), x = GB_feature_imp.sort_values('Importance', ascending = False).index, y = 'Importance')\n",
    "plt.xticks(rotation = 75);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Classifier Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_cv = GradientBoostingClassifier(random_state = 101)\n",
    "\n",
    "GB_cv_scores = cross_validate(GB_cv, X_train, y_train, \n",
    "                              scoring = ['accuracy', 'precision','recall', 'f1', 'roc_auc'], cv = 10)\n",
    "GB_cv_scores = pd.DataFrame(GB_cv_scores, index = range(1, 11))\n",
    "\n",
    "GB_cv_scores.mean()[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Classifier GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"n_estimators\":[100, 200, 300],\n",
    "              \"subsample\":[0.5, 1], \n",
    "              \"max_features\" : [None, 2, 3, 4],\n",
    "              \"learning_rate\": [0.001, 0.01, 0.1], \n",
    "              'max_depth':[3, 4, 5, 6]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_grid = GradientBoostingClassifier(random_state = 101)\n",
    "GB_grid_model = GridSearchCV(GB_grid, param_grid, scoring = \"f1\", verbose = 2, n_jobs = -1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_grid_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored('\\033[1mBest Parameters of GridSearchCV for Gradient Boosting Model:\\033[0m', 'blue'), colored(GB_grid_model.best_params_, 'cyan'))\n",
    "print(\"--------------------------------------------------------------------------------------------------------------------\")\n",
    "print(colored('\\033[1mBest Estimator of GridSearchCV for Gradient Boosting Model:\\033[0m', 'blue'), colored(GB_grid_model.best_estimator_, 'cyan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_tuned = GradientBoostingClassifier(learning_rate = 0.01,\n",
    "                                      max_depth = 6, \n",
    "                                      n_estimators = 200,\n",
    "                                      subsample = 0.5,\n",
    "                                      random_state = 101).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = GB_tuned.predict(X_test)\n",
    "y_train_pred = GB_tuned.predict(X_train)\n",
    "\n",
    "GB_tuned_f1 = f1_score(y_test, y_pred)\n",
    "GB_tuned_acc = accuracy_score(y_test, y_pred)\n",
    "GB_tuned_recall = recall_score(y_test, y_pred)\n",
    "GB_tuned_auc = roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GB_tuned\")\n",
    "print (\"------------------\")\n",
    "eval(GB_tuned, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val(y_train, y_train_pred, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassPredictionError\n",
    "\n",
    "visualizer = ClassPredictionError(GB_tuned)\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "visualizer.poof();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(GB_model, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(GB_model, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90HfPd4w2sn1"
   },
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_Pred = {\"Actual\": y_test, \"GB_Pred\":y_pred}\n",
    "GB_Pred = pd.DataFrame.from_dict(GB_Pred)\n",
    "GB_Pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Preds = GB_Pred\n",
    "Model_Preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient_boosting_classifier = pickle.dump(GB_tuned, open('gradient_boosting_model', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9P157eX2sn2"
   },
   "source": [
    "### KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPakx2ON2sn3"
   },
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_model = KNeighborsClassifier(n_neighbors = 5)\n",
    "KNN_model.fit(X_train, y_train)\n",
    "y_pred = KNN_model.predict(X_test)\n",
    "y_train_pred = KNN_model.predict(X_train)\n",
    "\n",
    "KNN_model_f1 = f1_score(y_test, y_pred)\n",
    "KNN_model_acc = accuracy_score(y_test, y_pred)\n",
    "KNN_model_recall = recall_score(y_test, y_pred)\n",
    "KNN_model_auc = roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KNN_Model\")\n",
    "print (\"------------------\")\n",
    "eval(KNN_model, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val(y_train, y_train_pred, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassPredictionError\n",
    "\n",
    "visualizer = ClassPredictionError(KNN_model)\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "visualizer.poof();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNeighbors Classifier Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_cv = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "KNN_cv_scores = cross_validate(KNN_cv, X_train, y_train, \n",
    "                              scoring = ['accuracy', 'precision','recall', 'f1', 'roc_auc'], cv = 10)\n",
    "KNN_cv_scores = pd.DataFrame(KNN_cv_scores, index = range(1, 11))\n",
    "\n",
    "KNN_cv_scores.mean()[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow Method for Choosing Reasonable K Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_rates = []\n",
    "\n",
    "for k in range(1, 30):\n",
    "    KNN = KNeighborsClassifier(n_neighbors = k)\n",
    "    KNN.fit(X_train, y_train) \n",
    "   \n",
    "    y_pred = KNN.predict(X_test)\n",
    "    \n",
    "    test_error = 1 - accuracy_score(y_test, y_pred)\n",
    "    test_error_rates.append(test_error)\n",
    "    \n",
    "print(test_error_rates)\n",
    "\n",
    "plt.figure(figsize = (15, 8))\n",
    "plt.plot(range(1, 30), test_error_rates, color = 'blue', linestyle = '--', marker = 'o',\n",
    "         markerfacecolor = 'red', markersize = 10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K_values')\n",
    "plt.ylabel('Error Rate')\n",
    "\n",
    "plt.hlines(y = 0.04307948860478039, xmin = 0, xmax = 30, colors = 'r', linestyles = \"--\", label = \"K-Value = 2 Line\")\n",
    "plt.hlines(y = 0.04558087826570312, xmin = 0, xmax = 30, colors = 'r', linestyles = \"--\", label = \"K-Value = 4 Line\")\n",
    "plt.hlines(y = 0.055864369093941, xmin = 0, xmax = 30, colors = 'blue', linestyles = \"--\", label = \"Default K-Value = 5 Line\")\n",
    "plt.legend(prop = {\"size\":14});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST A QUICK COMPARISON TO OUR DEFAULT K=5\n",
    "\n",
    "knn5 = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "knn5.fit(X_train,y_train)\n",
    "pred = knn5.predict(X_test)\n",
    "\n",
    "print('WITH K=5')\n",
    "print('---------------')\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print('---------------')\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW K=2\n",
    "\n",
    "knn2 = KNeighborsClassifier(n_neighbors = 2)\n",
    "\n",
    "knn2.fit(X_train,y_train)\n",
    "pred = knn2.predict(X_test)\n",
    "\n",
    "print('WITH K=2')\n",
    "print('---------------')\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print('---------------')\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW K=4\n",
    "\n",
    "knn4 = KNeighborsClassifier(n_neighbors = 4)\n",
    "\n",
    "knn4.fit(X_train,y_train)\n",
    "pred = knn4.predict(X_test)\n",
    "\n",
    "print('WITH K=4')\n",
    "print('---------------')\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print('---------------')\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW K=6\n",
    "\n",
    "knn6 = KNeighborsClassifier(n_neighbors = 6)\n",
    "\n",
    "knn6.fit(X_train,y_train)\n",
    "pred = knn6.predict(X_test)\n",
    "\n",
    "print('WITH K=6')\n",
    "print('---------------')\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print('---------------')\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW K=8\n",
    "\n",
    "knn8 = KNeighborsClassifier(n_neighbors = 8)\n",
    "\n",
    "knn8.fit(X_train,y_train)\n",
    "pred = knn8.predict(X_test)\n",
    "\n",
    "print('WITH K=8')\n",
    "print('---------------')\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print('---------------')\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW K=10\n",
    "\n",
    "knn10 = KNeighborsClassifier(n_neighbors = 10)\n",
    "\n",
    "knn10.fit(X_train,y_train)\n",
    "pred = knn10.predict(X_test)\n",
    "\n",
    "print('WITH K=10')\n",
    "print('---------------')\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print('---------------')\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNeighbors Classifier GridsearchCV for Choosing Reasonable K Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(1, 30)\n",
    "param_grid = {\"n_neighbors\": k_values, \"p\": [1, 2], \"weights\": ['uniform', \"distance\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_grid = KNeighborsClassifier()\n",
    "KNN_grid_model = GridSearchCV(KNN_grid, param_grid, cv = 10, scoring = 'recall')  # scoring RECALL olmalı mı?\n",
    "KNN_grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_grid_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored('\\033[1mBest Parameters of GridSearchCV for KNN Model:\\033[0m', 'blue'), colored(KNN_grid_model.best_params_, 'cyan'))\n",
    "print(\"--------------------------------------------------------------------------------------------------------------------\")\n",
    "print(colored('\\033[1mBest Estimator of GridSearchCV for KNN Model:\\033[0m', 'blue'), colored(KNN_grid_model.best_estimator_, 'cyan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW WITH K=3\n",
    "\n",
    "KNN_tuned3 = KNeighborsClassifier(n_neighbors = 3, p = 1)\n",
    "KNN_tuned3.fit(X_train, y_train)\n",
    "y_pred = KNN_tuned3.predict(X_test)\n",
    "y_train_pred = KNN_tuned3.predict(X_train)\n",
    "\n",
    "KNN_tuned3_f1 = f1_score(y_test, y_pred)\n",
    "KNN_tuned3_acc = accuracy_score(y_test, y_pred)\n",
    "KNN_tuned3_recall = recall_score(y_test, y_pred)\n",
    "KNN_tuned3_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"KNN_tuned (K=3)\")\n",
    "print (\"------------------\")\n",
    "eval(KNN_tuned3, X_train, X_test)\n",
    "train_val(y_train, y_train_pred, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW WITH K=1\n",
    "\n",
    "KNN_tuned1 = KNeighborsClassifier(n_neighbors = 1, p = 1)\n",
    "KNN_tuned1.fit(X_train, y_train)\n",
    "y_pred = KNN_tuned1.predict(X_test)\n",
    "y_train_pred = KNN_tuned1.predict(X_train)\n",
    "\n",
    "KNN_tuned1_f1 = f1_score(y_test, y_pred)\n",
    "KNN_tuned1_acc = accuracy_score(y_test, y_pred)\n",
    "KNN_tuned1_recall = recall_score(y_test, y_pred)\n",
    "KNN_tuned1_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"KNN_tuned (K=1)\")\n",
    "print (\"------------------\")\n",
    "eval(KNN_tuned1, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# burada 3 ve 1 aynı skorları veriyor. KNN_tuned olarak komleksiti seviyesini azaltmak için K=1 tuned olarak seçilip devam edilmeli mi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNeighbors Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(KNN_model, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(KNN_model, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BYTdWlr2soJ"
   },
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_Pred = {\"Actual\": y_test, \"KNN_Pred\":y_pred}\n",
    "KNN_Pred = pd.DataFrame.from_dict(KNN_Pred)\n",
    "KNN_Pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_Pred.drop(\"Actual\", axis = 1, inplace = True)\n",
    "Model_Preds = pd.merge(Model_Preds, KNN_Pred, left_index = True, right_index = True)\n",
    "Model_Preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kneighbors_classifier = pickle.dump(KNN_tuned3, open('kneighbors_model', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfMy1D_p2soK"
   },
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4GifMUw2soL"
   },
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = RandomForestClassifier(class_weight = \"balanced\", random_state = 101)\n",
    "RF_model.fit(X_train, y_train)\n",
    "y_pred = RF_model.predict(X_test)\n",
    "y_train_pred = RF_model.predict(X_train)\n",
    "\n",
    "RF_model_f1 = f1_score(y_test, y_pred)\n",
    "RF_model_acc = accuracy_score(y_test, y_pred)\n",
    "RF_model_recall = recall_score(y_test, y_pred)\n",
    "RF_model_auc = roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRUPvrME2soc"
   },
   "source": [
    "#### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXKuonpN2soe"
   },
   "outputs": [],
   "source": [
    "print(\"RF_Model\")\n",
    "print (\"------------------\")\n",
    "eval(RF_model, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val(y_train, y_train_pred, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassPredictionError\n",
    "\n",
    "visualizer = ClassPredictionError(RF_model)\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "visualizer.poof();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance for Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_feature_imp = pd.DataFrame(index=X.columns, data = RF_model.feature_importances_, columns = ['Importance']).sort_values(\"Importance\", ascending = False)\n",
    "RF_feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "sns.barplot(data = RF_feature_imp.sort_values('Importance', ascending = False), x = RF_feature_imp.sort_values('Importance', ascending = False).index, y = 'Importance')\n",
    "plt.xticks(rotation = 75);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_cv = RandomForestClassifier(class_weight = \"balanced\", random_state = 101)\n",
    "\n",
    "RF_cv_scores = cross_validate(RF_cv, X_train, y_train, \n",
    "                              scoring = ['accuracy', 'precision','recall', 'f1', 'roc_auc'], cv = 10)\n",
    "RF_cv_scores = pd.DataFrame(RF_cv_scores, index = range(1, 11))\n",
    "\n",
    "RF_cv_scores.mean()[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators' : [50, 100, 300],\n",
    "             'max_features' : [2, 3, 4],\n",
    "             'max_depth' : [3, 5, 7, 9],\n",
    "             'min_samples_split' : [2, 5, 8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_grid = RandomForestClassifier(class_weight = 'balanced', random_state = 101)\n",
    "RF_grid_model = GridSearchCV(estimator = RF_grid, \n",
    "                             param_grid = param_grid, \n",
    "                             scoring = \"recall\", \n",
    "                             n_jobs = -1, verbose = 2)\n",
    "RF_grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_grid_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored('\\033[1mBest Parameters of GridSearchCV for Random Forest Model:\\033[0m', 'blue'), colored(RF_grid_model.best_params_, 'cyan'))\n",
    "print(\"--------------------------------------------------------------------------------------------------------------------\")\n",
    "print(colored('\\033[1mBest Estimator of GridSearchCV for Random Forest Model:\\033[0m', 'blue'), colored(RF_grid_model.best_estimator_, 'cyan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_tuned = RandomForestClassifier(class_weight = 'balanced',\n",
    "                                  max_depth = 3,\n",
    "                                  max_features = 4,\n",
    "                                  n_estimators = 300,\n",
    "                                  random_state = 101).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = RF_tuned.predict(X_test)\n",
    "y_train_pred = RF_tuned.predict(X_train)\n",
    "\n",
    "RF_tuned_f1 = f1_score(y_test, y_pred)\n",
    "RF_tuned_acc = accuracy_score(y_test, y_pred)\n",
    "RF_tuned_recall = recall_score(y_test, y_pred)\n",
    "RF_tuned_auc = roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RF_tuned\")\n",
    "print (\"------------------\")\n",
    "eval(RF_tuned, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val(y_train, y_train_pred, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassPredictionError\n",
    "\n",
    "visualizer = ClassPredictionError(RF_tuned)\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "visualizer.poof();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(RF_model, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(RF_model, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKLtTwJ82som"
   },
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_Pred = {\"Actual\": y_test, \"RF_Pred\":y_pred}\n",
    "RF_Pred = pd.DataFrame.from_dict(RF_Pred)\n",
    "RF_Pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_Pred.drop(\"Actual\", axis = 1, inplace = True)\n",
    "Model_Preds = pd.merge(Model_Preds, RF_Pred, left_index = True, right_index = True)\n",
    "Model_Preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_forest_classifier = pickle.dump(RF_tuned, open('random_forest_model', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_model = CatBoostClassifier(verbose = False, scale_pos_weight = 4, random_state = 101)\n",
    "CB_model.fit(X_train, y_train)\n",
    "y_pred = CB_model.predict(X_test)\n",
    "y_train_pred = CB_model.predict(X_train)\n",
    "\n",
    "CB_model_f1 = f1_score(y_test, y_pred)\n",
    "CB_model_acc = accuracy_score(y_test, y_pred)\n",
    "CB_model_recall = recall_score(y_test, y_pred)\n",
    "CB_model_auc = roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CB_Model\")\n",
    "print (\"------------------\")\n",
    "eval(CB_model, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val(y_train, y_train_pred, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from yellowbrick.classifier import ClassPredictionError\n",
    "\n",
    "# visualizer = ClassPredictionError(CB_model)\n",
    "\n",
    "# # Fit the training data to the visualizer\n",
    "# visualizer.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model on the test data\n",
    "# visualizer.score(X_test, y_test)\n",
    "\n",
    "# # Draw visualization\n",
    "# visualizer.poof();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance for CatBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_feature_imp = pd.DataFrame(index = X.columns, data = CB_model.feature_importances_, columns = ['Importance']).sort_values(\"Importance\", ascending = False)\n",
    "CB_feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "sns.barplot(data = CB_feature_imp.sort_values('Importance', ascending = False), x = CB_feature_imp.sort_values('Importance', ascending = False).index, y = 'Importance')\n",
    "plt.xticks(rotation = 75);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost Classifier Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_cv = CatBoostClassifier(verbose = False, scale_pos_weight = 4, random_state = 101)\n",
    "\n",
    "CB_cv_scores = cross_validate(CB_cv, X_train, y_train, \n",
    "                              scoring = ['accuracy', 'precision','recall', 'f1', 'roc_auc'], cv = 10)\n",
    "CB_cv_scores = pd.DataFrame(CB_cv_scores, index = range(1, 11))\n",
    "\n",
    "CB_cv_scores.mean()[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost Classifier GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'learning_rate': [0.01, 0.03, 0.1, 0.5],\n",
    "              'depth': [4, 6, 8, 10],\n",
    "              'l2_leaf_reg': [1, 3, 5, 7, 9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_grid = CatBoostClassifier(verbose = False, scale_pos_weight = 4, random_state = 101)\n",
    "CB_grid_model = GridSearchCV(estimator = CB_grid, \n",
    "                             param_grid = param_grid, \n",
    "                             scoring = \"recall\", \n",
    "                             n_jobs = -1, verbose = 2)\n",
    "CB_grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored('\\033[1mBest Parameters of GridSearchCV for Random Forest Model:\\033[0m', 'blue'), colored(CB_grid_model.best_params_, 'cyan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_tuned = CatBoostClassifier(verbose = False, \n",
    "                              scale_pos_weight = 4,\n",
    "                              depth = 4,\n",
    "                              l2_leaf_reg = 3,\n",
    "                              learning_rate = 0.01,\n",
    "                              random_state = 101).fit(X_train, y_train)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = CB_tuned.predict(X_test)\n",
    "y_train_pred = CB_tuned.predict(X_train)\n",
    "\n",
    "CB_tuned_f1 = f1_score(y_test, y_pred)\n",
    "CB_tuned_acc = accuracy_score(y_test, y_pred)\n",
    "CB_tuned_recall = recall_score(y_test, y_pred)\n",
    "CB_tuned_auc = roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CB_tuned\")\n",
    "print (\"------------------\")\n",
    "eval(CB_tuned, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val(y_train, y_train_pred, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_cm = confusion_matrix(y_test, y_pred)\n",
    "CB_cm_df = pd.DataFrame(CB_cm)\n",
    "CB_cm_df = CB_cm_df.rename(columns={0:\"Employee_Stayed\", 1:\"Employe_Left\"}, index={0:\"Employee_Stayed\", 1:\"Employe_Left\"})\n",
    "CB_cm_df[\"Total\"] = CB_cm_df[\"Employee_Stayed\"] + CB_cm_df[\"Employe_Left\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(CB_cm_df, x=\"Employee_Stayed\", y=\"Total\", color=\"Employe_Left\", title=\"CatBoost Confusion Matrix Distribution\")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Employees-Left                    Employees_Stayed\",\n",
    "    yaxis_title=\"The Number of Employees\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=14,\n",
    "        color=\"#7f7f7f\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(CB_model, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(CB_model, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_Pred.drop(\"Actual\", axis = 1, inplace = True)\n",
    "Model_Preds = pd.merge(Model_Preds, CB_Pred, left_index = True, right_index = True)\n",
    "Model_Preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_Pred = {\"Actual\": y_test, \"CB_Pred\":y_pred}\n",
    "CB_Pred = pd.DataFrame.from_dict(CB_Pred)\n",
    "CB_Pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Preds.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost_classifier = pickle.dump(CB_tuned, open('catboost_model', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Models Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare = pd.DataFrame({\"Model\": [\"GB_model\", \"GB_tuned\", \"KNN_Model\", \"KNN_tuned3\", \"RF_model\", \"RF_tuned\", \"CB_model\", \n",
    "                                  \"CB_tuned\"],\n",
    "                        \n",
    "                        \"F1_Score\": [GB_model_f1, GB_tuned_f1, KNN_model_f1, KNN_tuned3_f1, RF_model_f1, RF_tuned_f1, \n",
    "                                     CB_model_f1, CB_tuned_f1],\n",
    "                                                 \n",
    "                        \"Accuracy_Score\": [GB_model_acc, GB_tuned_acc, KNN_model_acc, KNN_tuned3_acc, RF_model_acc, \n",
    "                                           RF_tuned_acc, CB_model_acc, CB_tuned_acc],\n",
    "                        \n",
    "                        \"Recall_Score\": [GB_model_recall, GB_tuned_recall, KNN_model_recall, KNN_tuned3_recall, RF_model_recall, \n",
    "                                     RF_tuned_recall, CB_model_recall, CB_tuned_recall],\n",
    "                       \n",
    "                        \"ROC_AUC_Score\": [GB_model_auc, GB_tuned_auc, KNN_model_auc, KNN_tuned3_auc, RF_model_auc, \n",
    "                                          RF_tuned_auc, CB_model_auc, CB_tuned_auc]})\n",
    "\n",
    "compare = compare.sort_values(by=\"Recall_Score\", ascending=True)\n",
    "fig = px.bar(compare, x = \"Recall_Score\", y = \"Model\", title = \"Recall_Score\")\n",
    "fig.show()\n",
    "\n",
    "compare = compare.sort_values(by=\"F1_Score\", ascending=True)\n",
    "fig = px.bar(compare, x = \"F1_Score\", y = \"Model\", title = \"F1_Score\")\n",
    "fig.show()\n",
    "\n",
    "compare = compare.sort_values(by=\"Accuracy_Score\", ascending=True)\n",
    "fig = px.bar(compare, x = \"Accuracy_Score\", y = \"Model\", title = \"Accuracy_Score\")\n",
    "fig.show()\n",
    "\n",
    "compare = compare.sort_values(by=\"ROC_AUC_Score\", ascending=True)\n",
    "fig = px.bar(compare, x = \"ROC_AUC_Score\", y = \"Model\", title = \"ROC_AUC_Score\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hv7E8XsazFMM"
   },
   "source": [
    "## 6. Model Deployement\n",
    "\n",
    "- Save and Export the Model as .pkl\n",
    "- Save and Export Variables as .pkl \n",
    "---------\n",
    "\n",
    "You cooked the food in the kitchen and moved on to the serving stage. The question is how do you showcase your work to others? Model Deployement helps you showcase your work to the world and make better decisions with it. But, deploying a model can get a little tricky at times. Before deploying the model, many things such as data storage, preprocessing, model building and monitoring need to be studied. Streamlit is a popular open source framework used by data scientists for model distribution.\n",
    "\n",
    "Deployment of machine learning models, means making your models available to your other business systems. By deploying models, other systems can send data to them and get their predictions, which are in turn populated back into the company systems. Through machine learning model deployment, can begin to take full advantage of the model you built.\n",
    "\n",
    "Data science is concerned with how to build machine learning models, which algorithm is more predictive, how to design features, and what variables to use to make the models more accurate. However, how these models are actually used is often neglected. And yet this is the most important step in the machine learning pipline. Only when a model is fully integrated with the business systems, real values ​​can be extract from its predictions.\n",
    "\n",
    "After doing the following operations in this notebook, jump to new .py file and create your web app with Streamlit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5pwXBOkJPeM"
   },
   "source": [
    "### Save and Export the Model as .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7sGSN6RJR6V"
   },
   "source": [
    "### Save and Export Variables as .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WeQNcROJScb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aD6JV41czCKr"
   },
   "source": [
    "___\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Churn Prediction_Student_Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
